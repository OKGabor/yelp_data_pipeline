{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# Package imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import min, max, col, to_timestamp, datediff, when, lit, udf, array, struct, explode, collect_list, concat_ws, countDistinct, split, trim, desc, array_contains, when, array_contains, col\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YelpReviewsClearProcess\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of restaurant-related businesses found: 52268\n",
      "Number of reviews in final dataset: 4724471\n"
     ]
    }
   ],
   "source": [
    "# This process should be iteraded for each year of the dataset by changing the input files and the output file name\n",
    "\n",
    "# 1. Load business data\n",
    "business_df = spark.read.json('yelp_dataset15/yelp_academic_dataset_business.json')\n",
    "\n",
    "# Check the schema to determine category type\n",
    "category_type = business_df.schema['categories'].dataType\n",
    "\n",
    "# Create appropriate filter based on data type\n",
    "if str(category_type).startswith('ArrayType'):\n",
    "    # Array type categories\n",
    "    category_filter = \"array_contains(categories, 'Restaurants')\"\n",
    "else:\n",
    "    # String type categories\n",
    "    category_filter = \"categories like '%Restaurants%'\"\n",
    "\n",
    "# Apply the appropriate filter\n",
    "restaurant_business_df = (business_df\n",
    "    .select('business_id', 'categories', 'latitude', 'longitude')\n",
    "    .where('categories is not null')\n",
    "    .where(category_filter)\n",
    "    .select('business_id', 'latitude', 'longitude')\n",
    ")\n",
    "\n",
    "# Cache the filtered business IDs since we'll reuse them\n",
    "restaurant_business_df.cache()\n",
    "\n",
    "# 2. Read and filter the reviews\n",
    "reviews_df = (spark.read.json('yelp_dataset15/yelp_academic_dataset_review.json')\n",
    "    .select('business_id', 'review_id', 'date', 'text', 'stars')\n",
    ")\n",
    "\n",
    "# 3. Join the data and create final dataset\n",
    "restaurant_data_df = (reviews_df\n",
    "    .join(restaurant_business_df, 'business_id')\n",
    "    .select(\n",
    "        'business_id',\n",
    "        'review_id',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'date',\n",
    "        'text',\n",
    "        'stars'\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. Write to parquet with optimization\n",
    "restaurant_data_df.coalesce(1).write.mode('overwrite').parquet('restaurant_data_15')\n",
    "\n",
    "# 5. Print some statistics\n",
    "print(f\"Number of restaurant-related businesses found: {restaurant_business_df.count()}\")\n",
    "print(f\"Number of reviews in final dataset: {restaurant_data_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial row count for restaurant reviews: 25582834\n",
      "Row count after removing duplicates: 14898440\n",
      "Removed 10684394 duplicate rows\n",
      "Final row count after removing null values: 14898440\n",
      "Removed 0 rows with null values\n",
      "+-----------+---------+--------+---------+----+----+-----+\n",
      "|business_id|review_id|latitude|longitude|date|text|stars|\n",
      "+-----------+---------+--------+---------+----+----+-----+\n",
      "|          0|        0|       0|        0|   0|   0|    0|\n",
      "+-----------+---------+--------+---------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"business_id\", StringType(), True),\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"stars\", DoubleType(), True)  # Explicitly set as DoubleType\n",
    "])\n",
    "\n",
    "# Read files individually and union them with schema conversion\n",
    "dfs = []\n",
    "for file in [\n",
    "    \"restaurant_data_10.snappy.parquet\",\n",
    "    \"restaurant_data_11.snappy.parquet\",\n",
    "    \"restaurant_data_12.snappy.parquet\",\n",
    "    \"restaurant_data_13.snappy.parquet\",\n",
    "    \"restaurant_data_14.snappy.parquet\",\n",
    "    \"restaurant_data_15.snappy.parquet\"\n",
    "]:\n",
    "    # Read each file and cast stars to double\n",
    "    df = spark.read.parquet(file)\n",
    "    df = df.withColumn(\"stars\", col(\"stars\").cast(\"double\"))\n",
    "    dfs.append(df)\n",
    "\n",
    "# Union all dataframes\n",
    "merged_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged_df = merged_df.union(df)\n",
    "\n",
    "# Initial count\n",
    "initial_count = merged_df.count()\n",
    "print(f\"Initial row count for restaurant reviews: {initial_count}\")\n",
    "\n",
    "# Remove duplicates\n",
    "deduplicated_df = merged_df.dropDuplicates()\n",
    "after_dedup_count = deduplicated_df.count()\n",
    "print(f\"Row count after removing duplicates: {after_dedup_count}\")\n",
    "print(f\"Removed {initial_count - after_dedup_count} duplicate rows\")\n",
    "\n",
    "# Drop rows with null values in critical columns\n",
    "cleaned_df = (deduplicated_df\n",
    "    .dropna(subset=['review_id', 'text', 'latitude', 'longitude', 'date'])\n",
    ")\n",
    "\n",
    "final_count = cleaned_df.count()\n",
    "print(f\"Final row count after removing null values: {final_count}\")\n",
    "print(f\"Removed {after_dedup_count - final_count} rows with null values\")\n",
    "\n",
    "# Get detailed null counts per column for reporting\n",
    "null_counts = cleaned_df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    for c in cleaned_df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined data to a new parquet file\n",
    "cleaned_df.coalesce(1).write.mode('overwrite').parquet('combined_restaurant_data_no_duplicates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximity and incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Additional cleaning\n",
    "whole_df = spark.read.parquet('combined_restaurant_data_no_duplicates.snappy.parquet')\n",
    "whole_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates based on business_id, review_id, and text: 3732283\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = cleaned_df.groupBy('business_id', 'review_id', 'text').count().filter('count > 1').count()\n",
    "print(f\"Number of duplicates based on business_id, review_id, and text: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10023390\n"
     ]
    }
   ],
   "source": [
    "deduplicated_df = whole_df.dropDuplicates(['business_id', 'review_id', 'text'])\n",
    "print(deduplicated_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 49857)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, when, explode, to_timestamp, collect_list, concat_ws\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, TimestampType, ArrayType, IntegerType\n",
    ")\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# User-configurable parameters\n",
    "VICTIMS_CUTOFF = 3\n",
    "PROXIMITY_DISTANCE_KM = 5\n",
    "REVIEW_TIMELINE_DAYS = 365\n",
    "\n",
    "# Function to calculate distance using the Haversine formula\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# Register UDF for Haversine distance calculation\n",
    "spark.udf.register(\"haversine_distance\", haversine_distance, DoubleType())\n",
    "\n",
    "# Load and preprocess reviews data, starting from deduplicated_df\n",
    "reviews_df = deduplicated_df.withColumn(\"date\", to_timestamp(\"date\"))\n",
    "\n",
    "# Load and preprocess shootings data\n",
    "shootings_df = spark.read.csv(\"/Users/okovacsgabor/local_documents/thesis_data/updated_shootings_with_coordinates.csv\", header=True, inferSchema=True) \\\n",
    "    .withColumn(\"Incident Date\", to_timestamp(\"Incident Date\", \"MMMM d, yyyy\")) \\\n",
    "    .filter(col(\"Victims Killed\") >= VICTIMS_CUTOFF)\n",
    "\n",
    "# Broadcast shootings data to all nodes\n",
    "broadcast_shootings = spark.sparkContext.broadcast(shootings_df.collect())\n",
    "\n",
    "# Define UDF to check proximity and time conditions\n",
    "def check_proximity_and_time_with_incidents(review_lat, review_lon, review_date):\n",
    "    if review_lat is None or review_lon is None or review_date is None:\n",
    "        return []\n",
    "    \n",
    "    incident_data = []\n",
    "    for shooting in broadcast_shootings.value:\n",
    "        if shooting[\"Latitude\"] is None or shooting[\"Longitude\"] is None:\n",
    "            continue\n",
    "        \n",
    "        distance = haversine_distance(\n",
    "            float(review_lat), float(review_lon),\n",
    "            float(shooting[\"Latitude\"]), float(shooting[\"Longitude\"])\n",
    "        )\n",
    "        \n",
    "        if distance <= PROXIMITY_DISTANCE_KM:\n",
    "            days_diff = (review_date - shooting[\"Incident Date\"]).days\n",
    "            if abs(days_diff) <= REVIEW_TIMELINE_DAYS:\n",
    "                weeks_diff = days_diff // 7\n",
    "                incident_data.append({\n",
    "                    \"incident_id\": str(shooting[\"Incident ID\"]),\n",
    "                    \"weeks_diff\": weeks_diff\n",
    "                })\n",
    "    \n",
    "    return incident_data\n",
    "\n",
    "# Register UDF\n",
    "from pyspark.sql.functions import udf\n",
    "check_proximity_and_time_with_incidents_udf = udf(\n",
    "    check_proximity_and_time_with_incidents,\n",
    "    ArrayType(StructType([\n",
    "        StructField(\"incident_id\", StringType()),\n",
    "        StructField(\"weeks_diff\", IntegerType())\n",
    "    ]))\n",
    ")\n",
    "\n",
    "# Apply UDF to create incident data column\n",
    "result_df = reviews_df.withColumn(\n",
    "    \"incident_data\",\n",
    "    check_proximity_and_time_with_incidents_udf(col(\"latitude\"), col(\"longitude\"), col(\"date\"))\n",
    ")\n",
    "\n",
    "# Explode and process incident data\n",
    "exploded_df = result_df.withColumn(\"incident_info\", explode(\"incident_data\")) \\\n",
    "    .withColumn(\"incident_id\", col(\"incident_info.incident_id\")) \\\n",
    "    .withColumn(\"weeks_diff\", col(\"incident_info.weeks_diff\")) \\\n",
    "    .drop(\"incident_data\", \"incident_info\")\n",
    "\n",
    "# Aggregate incident IDs and week differences\n",
    "grouped_df = exploded_df.groupBy(\"business_id\", \"review_id\", \"latitude\", \"longitude\", \"date\", \"text\", \"stars\") \\\n",
    "    .agg(collect_list(\"incident_id\").alias(\"incident_ids\"),\n",
    "         collect_list(\"weeks_diff\").alias(\"weeks_diffs\"))\n",
    "\n",
    "# Convert lists to strings\n",
    "final_df_with_string = grouped_df.withColumn(\"incident_ids_str\", concat_ws(\",\", col(\"incident_ids\"))) \\\n",
    "                                 .withColumn(\"weeks_diffs_str\", concat_ws(\",\", col(\"weeks_diffs\").cast(\"array<string>\")))\n",
    "\n",
    "print(final_df_with_string.count())\n",
    "\n",
    "# Write to Parquet\n",
    "final_df_with_string.select(\n",
    "    \"business_id\", \"review_id\", \"latitude\", \"longitude\", \"date\", \"text\", \"stars\", \"incident_ids_str\", \"weeks_diffs_str\"\n",
    ").coalesce(1).write.parquet(\"proximity_5.parquet\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding and filtering dataframe for week number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = spark.read.parquet(\"/Users/okovacsgabor/local_documents/thesis_data/clear_process/proximity_5.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+----------+-------------------+--------------------+-----+-----------+----------+\n",
      "|         business_id|           review_id| latitude| longitude|               date|                text|stars|incident_id|weeks_diff|\n",
      "+--------------------+--------------------+---------+----------+-------------------+--------------------+-----+-----------+----------+\n",
      "|--9e1ONYQuAa-CB_R...|-h-J6d7TyoKtWMyuP...|36.123183|-115.16919|2018-01-30 20:03:45|Top notch service...|  5.0|     946496|      17.0|\n",
      "|--9e1ONYQuAa-CB_R...|2Sk-2jlmv0jj4RjGi...|36.123183|-115.16919|2017-04-23 00:00:00|Ask ten local 'fo...|  4.0|     946496|     -23.0|\n",
      "|--9e1ONYQuAa-CB_R...|6D-KwWsfVd25r4EYa...|36.123183|-115.16919|2017-12-29 21:49:50|TexasTripletMom's...|  4.0|     946496|      12.0|\n",
      "|--9e1ONYQuAa-CB_R...|C-bI45l6hz8fvX-Nc...|36.123183|-115.16919|2016-12-31 00:00:00|It was above aver...|  4.0|     946496|     -40.0|\n",
      "|--9e1ONYQuAa-CB_R...|F1NfXHeVrS2i9y17y...|36.123183|-115.16919|2017-07-28 15:32:44|Came here for our...|  5.0|     946496|     -10.0|\n",
      "|--9e1ONYQuAa-CB_R...|HXIAQfUsX40kFNWyN...|36.123183|-115.16919|2017-02-04 14:39:29|Everything we exp...|  5.0|     946496|     -35.0|\n",
      "|--9e1ONYQuAa-CB_R...|Hs29e4KpVuOR9PrUm...|36.123183|-115.16919|2017-12-25 00:00:00|We celebrated our...|  5.0|     946496|      12.0|\n",
      "|--9e1ONYQuAa-CB_R...|JCFj_-YnoUjJhyif-...|36.123183|-115.16919|2016-12-03 00:00:00|Great service fro...|  4.0|     946496|     -44.0|\n",
      "|--9e1ONYQuAa-CB_R...|LBm5uRfRB02bdAIiX...|36.123183|-115.16919|2017-05-10 02:58:07|Meh. Just .. meh....|  3.0|     946496|     -21.0|\n",
      "|--9e1ONYQuAa-CB_R...|LXg4iJU5y9C0BbeWc...|36.123183|-115.16919|2017-03-08 23:07:00|The entire experi...|  5.0|     946496|     -30.0|\n",
      "|--9e1ONYQuAa-CB_R...|LsXwIxjJObNWb8mOQ...|36.123183|-115.16919|2017-10-05 00:00:00|Nice steakhouse l...|  4.0|     946496|       0.0|\n",
      "|--9e1ONYQuAa-CB_R...|OygdoKwPk1-RbRcyd...|36.123183|-115.16919|2017-12-08 02:46:57|Seriously one of ...|  5.0|     946496|       9.0|\n",
      "|--9e1ONYQuAa-CB_R...|PKGHDuKRHKU6-5zps...|36.123183|-115.16919|2018-02-23 18:31:12|this is the best ...|  5.0|     946496|      20.0|\n",
      "|--9e1ONYQuAa-CB_R...|RCP8GyOFMWzvNkFl1...|36.123183|-115.16919|2017-11-12 00:00:00|Where I became of...|  5.0|     946496|       6.0|\n",
      "|--9e1ONYQuAa-CB_R...|SmcUTvp2g1826oi0h...|36.123183|-115.16919|2018-06-22 23:06:38|So Delmonicos ins...|  4.0|     946496|      37.0|\n",
      "|--9e1ONYQuAa-CB_R...|UrK5S33NN6kEjmpdr...|36.123183|-115.16919|2017-10-17 00:00:00|We were disappoin...|  2.0|     946496|       2.0|\n",
      "|--9e1ONYQuAa-CB_R...|UweVOeA8Stu5LxR3i...|36.123183|-115.16919|2018-03-06 04:01:46|Amazing food, gre...|  5.0|     946496|      22.0|\n",
      "|--9e1ONYQuAa-CB_R...|VL1e4ZrVH_21SfWeu...|36.123183|-115.16919|2017-08-30 20:40:00|Went here for my ...|  5.0|     946496|      -5.0|\n",
      "|--9e1ONYQuAa-CB_R...|ZhmTixtNjIgUGF4LL...|36.123183|-115.16919|2016-11-05 00:00:00|Wow, we had the b...|  5.0|     946496|     -48.0|\n",
      "|--9e1ONYQuAa-CB_R...|aGfG15DGP_zNOjF9A...|36.123183|-115.16919|2018-04-30 00:00:00|I have been to al...|  5.0|     946496|      30.0|\n",
      "+--------------------+--------------------+---------+----------+-------------------+--------------------+-----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, posexplode, col\n",
    "\n",
    "# Split the incident_ids_str and weeks_diffs_str columns into arrays\n",
    "test_df = test_df.withColumn(\"incident_ids_array\", split(test_df[\"incident_ids_str\"], \",\")) \\\n",
    "       .withColumn(\"weeks_diffs_array\", split(test_df[\"weeks_diffs_str\"], \",\"))\n",
    "\n",
    "# Explode the incident_ids_array and weeks_diffs_array columns\n",
    "exploded_df = test_df.selectExpr(\"*\", \"posexplode(incident_ids_array) as (pos, incident_id)\") \\\n",
    "                .selectExpr(\"*\", \"posexplode(weeks_diffs_array) as (pos2, weeks_diff)\") \\\n",
    "                .filter(col(\"pos\") == col(\"pos2\")) \\\n",
    "                .drop(\"pos\", \"pos2\", \"incident_ids_array\", \"weeks_diffs_array\", \"incident_ids_str\", \"weeks_diffs_str\")\n",
    "\n",
    "# Change weeks diff to double\n",
    "exploded_df = exploded_df.withColumn(\"weeks_diff\", col(\"weeks_diff\").cast(\"double\"))\n",
    "\n",
    "# Drop duplicates by business_id, review_id, and incident_id -- additional cleaning\n",
    "exploded_df = exploded_df.dropDuplicates([\"business_id\", \"review_id\", \"incident_id\"])\n",
    "\n",
    "# Show the result\n",
    "exploded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169668\n"
     ]
    }
   ],
   "source": [
    "# Filter the rows where the weeks_diff is within -8 and 8\n",
    "eight_df = exploded_df.filter((col(\"weeks_diff\") >= -8) & (col(\"weeks_diff\") <= 7))\n",
    "print(eight_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct shooting incidents in the data:\n",
      "50\n",
      "--------------------\n",
      "Distinct businesses in the data:\n",
      "10652\n"
     ]
    }
   ],
   "source": [
    "distinct_incident_ids = eight_df.select(countDistinct(\"incident_id\").alias(\"distinct_count\")).collect()[0][\"distinct_count\"]\n",
    "distinct_business_ids = eight_df.select(countDistinct(\"business_id\").alias(\"distinct_count\")).collect()[0][\"distinct_count\"]\n",
    "print(\"Distinct shooting incidents in the data:\")\n",
    "print(distinct_incident_ids)\n",
    "print(\"--------------------\")\n",
    "print(\"Distinct businesses in the data:\")\n",
    "print(distinct_business_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates based on review_id and business_id: 1800\n"
     ]
    }
   ],
   "source": [
    "duplicate_count = eight_df.groupBy('review_id', 'business_id').count().filter('count > 1').count()\n",
    "print(f\"Number of duplicates based on review_id and business_id: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+----------+-----------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+----------+\n",
      "|business_id           |review_id             |latitude  |longitude  |date               |text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |stars|incident_id|weeks_diff|\n",
      "+----------------------+----------------------+----------+-----------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+----------+\n",
      "|-8oIIpYJAXWg-G3glapMmQ|-jYZaiXAxmtd23uceIU5ag|39.7883472|-86.1376312|2021-03-07 17:32:45|I've wanted to come to Festiva for a while now. I am finally here and I don't want to leave! First, the aesthetic. I sat on the out door patio. It was a rustic vibe with music playing softly. I was shortly seated and I started with their corn chips drizzled in chili sauce. I was not a fan but it is different and I like that! I started with the Margarita Mejore. It was perfect! It very fresh and light. That drink being the encouragement behind the chair salsa. Haha! I followed that up with the spicy chips and salsa. If you like spicy these will light your tongue up! Okay and the star of the show, the shrimp tacos with a side of guacamole! It was served with mexican rice and I chose frijole charros. The shrimp was breaded and seasoned perfectly! I tell you, it was just bursting with flavor in each bite! I got the caramelized onions on the side but I would definitely get it as it comes next time. I loved the soft corn tortilla how it paired with the crunch of the shrimp. These tacos will fill you up. I was torn between the guac or the salsa as my appetizer so I got it as a side. I am happy I did because the guacamole was very simple although it seemed unique. But I cannot wait to come back! It just a chill place to come and enjoy great food and amazing margaritas. I will definitely come back to try the enchiladas, Queso and the next seasonal Margarita!|5.0  |1952161    |-1.0      |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|-jYZaiXAxmtd23uceIU5ag|39.7883472|-86.1376312|2021-03-07 17:32:45|I've wanted to come to Festiva for a while now. I am finally here and I don't want to leave! First, the aesthetic. I sat on the out door patio. It was a rustic vibe with music playing softly. I was shortly seated and I started with their corn chips drizzled in chili sauce. I was not a fan but it is different and I like that! I started with the Margarita Mejore. It was perfect! It very fresh and light. That drink being the encouragement behind the chair salsa. Haha! I followed that up with the spicy chips and salsa. If you like spicy these will light your tongue up! Okay and the star of the show, the shrimp tacos with a side of guacamole! It was served with mexican rice and I chose frijole charros. The shrimp was breaded and seasoned perfectly! I tell you, it was just bursting with flavor in each bite! I got the caramelized onions on the side but I would definitely get it as it comes next time. I loved the soft corn tortilla how it paired with the crunch of the shrimp. These tacos will fill you up. I was torn between the guac or the salsa as my appetizer so I got it as a side. I am happy I did because the guacamole was very simple although it seemed unique. But I cannot wait to come back! It just a chill place to come and enjoy great food and amazing margaritas. I will definitely come back to try the enchiladas, Queso and the next seasonal Margarita!|5.0  |1910136    |6.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|6Z1IOVEG7WZyJdqu0-uDoA|39.7883472|-86.1376312|2021-02-21 21:53:36|Very easy curbside system and the food was delicious! I had the chicken tinga enchiladas, and I highly recommend them!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |5.0  |1910136    |4.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|6Z1IOVEG7WZyJdqu0-uDoA|39.7883472|-86.1376312|2021-02-21 21:53:36|Very easy curbside system and the food was delicious! I had the chicken tinga enchiladas, and I highly recommend them!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |5.0  |1952161    |-3.0      |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|CJj4EdA9ZGkbTiMmczOblw|39.7883472|-86.1376312|2021-01-30 02:24:34|We have been to Festiva before, and it never disappoints. We tried the Devour (takeout) option, and the food was scrumptious. The margaritas and churros were amazing. I would highly recommend this eatery. The curbside service was prompt and adhered to all Covid-19 precautions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |5.0  |1952161    |-6.0      |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|CJj4EdA9ZGkbTiMmczOblw|39.7883472|-86.1376312|2021-01-30 02:24:34|We have been to Festiva before, and it never disappoints. We tried the Devour (takeout) option, and the food was scrumptious. The margaritas and churros were amazing. I would highly recommend this eatery. The curbside service was prompt and adhered to all Covid-19 precautions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |5.0  |1910136    |0.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|ZYCdIFO9RX3gprbIuOko6Q|39.7883472|-86.1376312|2021-02-28 02:27:19|This restaurant was everything we wanted and more!! They ask you to hand sanitize immediately walking in, and practice responsible social distancing. From there the service was amazing, the food was so fresh and inventive! Guacamole with pomegranates and pumpkin seeds, spiced season margaritas, and the best tacos al pastor I've ever had!!! A great hidden gem of Indy!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |5.0  |1952161    |-2.0      |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|ZYCdIFO9RX3gprbIuOko6Q|39.7883472|-86.1376312|2021-02-28 02:27:19|This restaurant was everything we wanted and more!! They ask you to hand sanitize immediately walking in, and practice responsible social distancing. From there the service was amazing, the food was so fresh and inventive! Guacamole with pomegranates and pumpkin seeds, spiced season margaritas, and the best tacos al pastor I've ever had!!! A great hidden gem of Indy!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |5.0  |1910136    |5.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|a0HTh5QO3Vayeip03wKiVQ|39.7883472|-86.1376312|2021-03-15 14:26:05|How have I never been here?! Such a cute little place. We sat out in their covered patio and loved it! We started off with the guac which was literally awesome! We also got the queso to start. It was good but different than traditional queso which I do love. More rotel like. We got a few pitchers of their house margs, very tasty! Last but not least I got incredible Baja fish tacos. 3 to an order and they all had such large pieces of fish. So so good and our waiter was fabulous and funny. Very personal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |5.0  |1910136    |7.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|a0HTh5QO3Vayeip03wKiVQ|39.7883472|-86.1376312|2021-03-15 14:26:05|How have I never been here?! Such a cute little place. We sat out in their covered patio and loved it! We started off with the guac which was literally awesome! We also got the queso to start. It was good but different than traditional queso which I do love. More rotel like. We got a few pitchers of their house margs, very tasty! Last but not least I got incredible Baja fish tacos. 3 to an order and they all had such large pieces of fish. So so good and our waiter was fabulous and funny. Very personal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |5.0  |1952161    |0.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|clY2EokKkY7gOE-ICfKL2w|39.7883472|-86.1376312|2021-02-12 21:40:32|What a cute little gem in Indy!! So impressed with the authentic Mexican style foods & drinks and the cozy, personal atmosphere. The staff was wonderful, so nice and explained it all during our first visit here!\\n\\nWe came for the devour menu and I would HIGHLY recommend. It comes with a margarita so you can't beat that. And, the margarita was so terrific I had to have another. \\nThe chips queso and salsa were great as well, I'm not a huge salsa person but this one was so worth it, especially with the warm queso.\\nWe both got chicken enchiladas for the main course, they were good, I'd get them again, but they weren't a standout because the rest of the mea was so delicious. \\nMy FAVORITE part was dessert. The chocolate flan cake was to die for, both my boyfriend and I said we could each eat another 2 pieces. It was the perfect balance of chocolate with flan - kinda tasted like chocolate cake topped with creme brûlée. Yum. \\n\\nHighly recommend, especially for devour for a terrific deal, we'll be back!                                                                                                                                                                                                                                                                                                                                                                     |5.0  |1910136    |2.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|clY2EokKkY7gOE-ICfKL2w|39.7883472|-86.1376312|2021-02-12 21:40:32|What a cute little gem in Indy!! So impressed with the authentic Mexican style foods & drinks and the cozy, personal atmosphere. The staff was wonderful, so nice and explained it all during our first visit here!\\n\\nWe came for the devour menu and I would HIGHLY recommend. It comes with a margarita so you can't beat that. And, the margarita was so terrific I had to have another. \\nThe chips queso and salsa were great as well, I'm not a huge salsa person but this one was so worth it, especially with the warm queso.\\nWe both got chicken enchiladas for the main course, they were good, I'd get them again, but they weren't a standout because the rest of the mea was so delicious. \\nMy FAVORITE part was dessert. The chocolate flan cake was to die for, both my boyfriend and I said we could each eat another 2 pieces. It was the perfect balance of chocolate with flan - kinda tasted like chocolate cake topped with creme brûlée. Yum. \\n\\nHighly recommend, especially for devour for a terrific deal, we'll be back!                                                                                                                                                                                                                                                                                                                                                                     |5.0  |1952161    |-5.0      |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|obNHsAEjqta38M6prh8EkQ|39.7883472|-86.1376312|2021-01-30 00:44:17|Holy Mole -- this place is great.  Came here for the devour Indy prix fix dinner.  Amazing chips and salsa appetizers (ate way too much of that) and then ordered the chicken mole enchiladas and the shrimp tacos.  \\n\\nThe chicken mole was not bad, but it was a little bland for my tastes.  But the shrimp tocas were great.  Light batter, lots of shrimp flavor.  They had a carnitas tacos in the menu that we almost went with, which I think would have been great as well.\\n\\nWe got the Flan for desert, which my girlfriend loved, I was really hoping for the churro but I gave in.\\n\\nThe margaritas were amazingly good as well.  Will definitely be returning to try other items in the menu.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |4.0  |1952161    |-6.0      |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|obNHsAEjqta38M6prh8EkQ|39.7883472|-86.1376312|2021-01-30 00:44:17|Holy Mole -- this place is great.  Came here for the devour Indy prix fix dinner.  Amazing chips and salsa appetizers (ate way too much of that) and then ordered the chicken mole enchiladas and the shrimp tacos.  \\n\\nThe chicken mole was not bad, but it was a little bland for my tastes.  But the shrimp tocas were great.  Light batter, lots of shrimp flavor.  They had a carnitas tacos in the menu that we almost went with, which I think would have been great as well.\\n\\nWe got the Flan for desert, which my girlfriend loved, I was really hoping for the churro but I gave in.\\n\\nThe margaritas were amazingly good as well.  Will definitely be returning to try other items in the menu.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |4.0  |1910136    |0.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|rSHHRjlGncsyJ1ebKzqP0w|39.7883472|-86.1376312|2021-03-15 22:29:18|If you are trying to get drunk on Sunday at 10 am this is definitely a place to go to! They got bottomless drinks to choose from and they definitely kept it bottomless. \\nThe food is great, service was fantastic and aesthetically great place to eat\\nThe enchiladas were amazing and so were the pozole that my friend had. \\nThe bottomless drinks were only $15 per person with an entree! \\nDefinitely worth it when you had 4-5 drinks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |5.0  |1910136    |7.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|rSHHRjlGncsyJ1ebKzqP0w|39.7883472|-86.1376312|2021-03-15 22:29:18|If you are trying to get drunk on Sunday at 10 am this is definitely a place to go to! They got bottomless drinks to choose from and they definitely kept it bottomless. \\nThe food is great, service was fantastic and aesthetically great place to eat\\nThe enchiladas were amazing and so were the pozole that my friend had. \\nThe bottomless drinks were only $15 per person with an entree! \\nDefinitely worth it when you had 4-5 drinks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |5.0  |1952161    |0.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|yGlZ3f2CZflk05j_eTvQZA|39.7883472|-86.1376312|2021-01-30 01:49:11|Festiva is such a hidden gem!! Just a few blocks from downtown off of 16th street and truly one of the best spots in Indy.\\n\\nOur new couple friends wanted to try somewhere new for DeVour Downtown and after reviewing the Festiva menu I knew we had to go. They take reservations on their website and I'd encourage you make one! I called today because there were no reservations left and the amazing guy who answered the phone got us in - talk about high quality service.\\n\\nWe had margaritas, chips with salsa and guacamole to start off our dinner. We ordered the vegetarian enchiladas and shrimp tacos for dinner. The shrimp tacos were seriously some of the best tacos I've ever had. Last but not least we finished with churros and homemade chocolate sauce.\\n\\nNeedless to say Festiva has moved to one of my favorite places in Indianapolis to eat. While the space isn't super huge it's so cozy and they even have a heated patio! If you are wanting some authentic Mexican food it's a must!                                                                                                                                                                                                                                                                                                                                                                                               |5.0  |1910136    |0.0       |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|yGlZ3f2CZflk05j_eTvQZA|39.7883472|-86.1376312|2021-01-30 01:49:11|Festiva is such a hidden gem!! Just a few blocks from downtown off of 16th street and truly one of the best spots in Indy.\\n\\nOur new couple friends wanted to try somewhere new for DeVour Downtown and after reviewing the Festiva menu I knew we had to go. They take reservations on their website and I'd encourage you make one! I called today because there were no reservations left and the amazing guy who answered the phone got us in - talk about high quality service.\\n\\nWe had margaritas, chips with salsa and guacamole to start off our dinner. We ordered the vegetarian enchiladas and shrimp tacos for dinner. The shrimp tacos were seriously some of the best tacos I've ever had. Last but not least we finished with churros and homemade chocolate sauce.\\n\\nNeedless to say Festiva has moved to one of my favorite places in Indianapolis to eat. While the space isn't super huge it's so cozy and they even have a heated patio! If you are wanting some authentic Mexican food it's a must!                                                                                                                                                                                                                                                                                                                                                                                               |5.0  |1952161    |-6.0      |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|zD5xdLFsIzVVen_eTjVVlw|39.7883472|-86.1376312|2021-01-23 16:05:13|If you drove past this place you probably would t think much of it. We're new to the neighborhood and thought we would finally give it a try and I'm so happy we did. Was in a complete food coma last night because I had to eat everything off our plates it was sooo good!\\n\\nWe started with a pepita (mezcal and cucumber) lord was that amazing. We got the stuffed poblano pepper, great spice, very good size and absolutely delicious. \\n\\nEntrees were the ribs and shrimp tacos. Like the other reviews on here - GET THE RIBS. Literally fall off the bone and the bonus of the side mixture it comes with (cole slaw like) pairs so well with it.\\n\\nShrimp tacos were to die for as well. Perfect amount of sauce with it, would get again.\\n\\nChurros - best churros I've had in Indy. We already talked about how we might just have to put in a to go order for them tonight \\n\\nVibes: small little place, good for date night or small group of friends. Make sure you make reservations, it was completely full on a Friday night                                                                                                                                                                                                                                                                                                                                                                      |5.0  |1952161    |-7.0      |\n",
      "|-8oIIpYJAXWg-G3glapMmQ|zD5xdLFsIzVVen_eTjVVlw|39.7883472|-86.1376312|2021-01-23 16:05:13|If you drove past this place you probably would t think much of it. We're new to the neighborhood and thought we would finally give it a try and I'm so happy we did. Was in a complete food coma last night because I had to eat everything off our plates it was sooo good!\\n\\nWe started with a pepita (mezcal and cucumber) lord was that amazing. We got the stuffed poblano pepper, great spice, very good size and absolutely delicious. \\n\\nEntrees were the ribs and shrimp tacos. Like the other reviews on here - GET THE RIBS. Literally fall off the bone and the bonus of the side mixture it comes with (cole slaw like) pairs so well with it.\\n\\nShrimp tacos were to die for as well. Perfect amount of sauce with it, would get again.\\n\\nChurros - best churros I've had in Indy. We already talked about how we might just have to put in a to go order for them tonight \\n\\nVibes: small little place, good for date night or small group of friends. Make sure you make reservations, it was completely full on a Friday night                                                                                                                                                                                                                                                                                                                                                                      |5.0  |1910136    |-1.0      |\n",
      "+----------------------+----------------------+----------+-----------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are still duplicates based on 'business_id' and 'review_id' caused by multiple incidents\n",
    "# Step 1: Identify duplicates based on 'business_id' and 'review_id'\n",
    "duplicates = (eight_df\n",
    "              .groupBy(\"business_id\", \"review_id\")\n",
    "              .count()\n",
    "              .filter(F.col(\"count\") > 1)\n",
    "              .select(\"business_id\", \"review_id\"))\n",
    "\n",
    "# Step 2: Join back with the original dataframe to get all columns for duplicate rows\n",
    "duplicate_rows = (eight_df\n",
    "                  .join(duplicates, on=[\"business_id\", \"review_id\"], how=\"inner\")\n",
    "                  .orderBy(\"business_id\", \"review_id\"))\n",
    "\n",
    "# Show the result\n",
    "duplicate_rows.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def handle_duplicates(eight_df, incidents_path):\n",
    "    # First, load the incidents data\n",
    "    incidents_df = spark.read.csv(incidents_path, header=True)\n",
    "    \n",
    "    # Convert necessary columns in incidents_df\n",
    "    incidents_df = incidents_df.withColumn(\n",
    "        \"Incident Date\", \n",
    "        F.to_timestamp(\"Incident Date\", \"MM/dd/yyyy\")\n",
    "    )\n",
    "    \n",
    "    # Create a window spec for finding the closest positive weeks_diff to 0\n",
    "    window_spec = Window.partitionBy(\"business_id\", \"review_id\")\n",
    "    \n",
    "    # Add row numbers based on our primary criterion:\n",
    "    # 1. Only positive weeks_diff\n",
    "    # 2. Closest to 0\n",
    "    with_ranks = eight_df.withColumn(\n",
    "        \"abs_weeks_diff\",\n",
    "        F.abs(\"weeks_diff\")\n",
    "    ).withColumn(\n",
    "        \"rank\",\n",
    "        F.row_number().over(\n",
    "            window_spec.orderBy(\n",
    "                # First ensure weeks_diff is positive\n",
    "                F.when(F.col(\"weeks_diff\") < 0, float('inf'))\n",
    "                .otherwise(F.col(\"abs_weeks_diff\"))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Get cases where rank = 1 (these are our clear winners)\n",
    "    clear_winners = with_ranks.filter(F.col(\"rank\") == 1)\n",
    "    \n",
    "    # Find cases where we have ties (same abs_weeks_diff for same business_id, review_id)\n",
    "    ties = with_ranks.withColumn(\n",
    "        \"min_abs_diff\",\n",
    "        F.min(\"abs_weeks_diff\").over(window_spec)\n",
    "    ).filter(\n",
    "        (F.col(\"abs_weeks_diff\") == F.col(\"min_abs_diff\")) &\n",
    "        (F.col(\"weeks_diff\") > 0)\n",
    "    ).withColumn(\n",
    "        \"tie_count\",\n",
    "        F.count(\"*\").over(window_spec)\n",
    "    ).filter(F.col(\"tie_count\") > 1)\n",
    "    \n",
    "    if ties.count() > 0:\n",
    "        # For ties, we need to consider incident dates\n",
    "        ties_with_dates = ties.join(\n",
    "            incidents_df.select(\n",
    "                F.col(\"Incident ID\").alias(\"incident_id\"),\n",
    "                \"Incident Date\"\n",
    "            ),\n",
    "            on=\"incident_id\"\n",
    "        )\n",
    "        \n",
    "        # Calculate time difference between review and incident\n",
    "        ties_with_dates = ties_with_dates.withColumn(\n",
    "            \"time_diff\",\n",
    "            F.abs(F.unix_timestamp(\"date\") - F.unix_timestamp(\"Incident Date\"))\n",
    "        )\n",
    "        \n",
    "        # Select the row with minimum time difference for each tie group\n",
    "        tie_winners = ties_with_dates.withColumn(\n",
    "            \"tie_rank\",\n",
    "            F.row_number().over(\n",
    "                window_spec.orderBy(\"time_diff\")\n",
    "            )\n",
    "        ).filter(F.col(\"tie_rank\") == 1)\n",
    "        \n",
    "        # Select only the original columns\n",
    "        tie_winners = tie_winners.select(eight_df.columns)\n",
    "        \n",
    "        # Combine clear winners with tie winners\n",
    "        final_df = clear_winners.filter(\n",
    "            ~F.concat(F.col(\"business_id\"), F.col(\"review_id\")).isin(\n",
    "                ties.select(\n",
    "                    F.concat(F.col(\"business_id\"), F.col(\"review_id\"))\n",
    "                ).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            )\n",
    "        ).union(tie_winners)\n",
    "    else:\n",
    "        final_df = clear_winners\n",
    "    \n",
    "    # Return final dataframe with only the original columns\n",
    "    return final_df.select(eight_df.columns)\n",
    "\n",
    "# Usage:\n",
    "incidents_path = \"/Users/okovacsgabor/local_documents/thesis_data/updated_shootings_with_coordinates.csv\"\n",
    "result_df = handle_duplicates(eight_df, incidents_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----+\n",
      "|business_id|review_id|count|\n",
      "+-----------+---------+-----+\n",
      "+-----------+---------+-----+\n",
      "\n",
      "Number of duplicates based on review_id and business_id: 0\n",
      "167866\n"
     ]
    }
   ],
   "source": [
    "# Check for any remaining duplicates\n",
    "remaining_duplicates = (result_df\n",
    "    .groupBy(\"business_id\", \"review_id\")\n",
    "    .count()\n",
    "    .filter(F.col(\"count\") > 1))\n",
    "\n",
    "remaining_duplicates.show()  # Should show 0 results\n",
    "\n",
    "duplicate_count = result_df.groupBy('review_id', 'business_id').count().filter('count > 1').count()\n",
    "print(f\"Number of duplicates based on review_id and business_id: {duplicate_count}\")\n",
    "print(result_df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------------+\n",
      "|distinct_weeks|num_restaurant_incident_pairs|\n",
      "+--------------+-----------------------------+\n",
      "|            16|                          509|\n",
      "|            15|                          336|\n",
      "|            14|                          309|\n",
      "|            13|                          367|\n",
      "|            12|                          465|\n",
      "|            11|                          474|\n",
      "|            10|                          525|\n",
      "|             9|                          637|\n",
      "|             8|                          616|\n",
      "|             7|                          746|\n",
      "|             6|                          927|\n",
      "|             5|                          966|\n",
      "|             4|                         1180|\n",
      "|             3|                         1565|\n",
      "|             2|                         2048|\n",
      "|             1|                         3297|\n",
      "+--------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by business_id and incident_id to count distinct weeks_diff for each combination\n",
    "incident_weeks_review_count = (\n",
    "    result_df\n",
    "    .groupBy(\"business_id\", \"incident_id\")  # Group by both business_id and incident_id\n",
    "    .agg(F.countDistinct(\"weeks_diff\").alias(\"distinct_weeks\"))  # Count distinct weeks for each combination\n",
    ")\n",
    "\n",
    "# Step 2: Group by distinct_weeks and count how many restaurant-incident pairs fall into each category\n",
    "incident_weeks_summary = (\n",
    "    incident_weeks_review_count\n",
    "    .groupBy(\"distinct_weeks\")\n",
    "    .agg(F.count(\"*\").alias(\"num_restaurant_incident_pairs\"))  # Count the number of restaurant-incident pairs for each distinct_weeks\n",
    "    .orderBy(\"distinct_weeks\", ascending=False)  # Sort by number of weeks in descending order\n",
    ")\n",
    "\n",
    "# Step 3: Show the result\n",
    "incident_weeks_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.coalesce(1).write.mode('overwrite').parquet('minus_8_plus_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining scores and filtering for englsih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = spark.read.parquet(\"/Users/okovacsgabor/local_documents/thesis_data/clear_process/minus_8_plus_7.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of reviews dropped: 0.19%\n"
     ]
    }
   ],
   "source": [
    "# Drop non english rows\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from langdetect import detect\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Step 1: Define a UDF for language detection\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None  # Return None if detection fails\n",
    "\n",
    "detect_language_udf = udf(detect_language, StringType())\n",
    "\n",
    "# Step 2: Apply the UDF to add a `language` column\n",
    "pre_df = pre_df.withColumn(\"language\", detect_language_udf(F.col(\"text\")))\n",
    "\n",
    "# Step 3: Filter only English reviews (where language is 'en')\n",
    "english_reviews_df = pre_df.filter(F.col(\"language\") == \"en\")\n",
    "\n",
    "# Step 4: Calculate the percentage of reviews dropped\n",
    "initial_count = pre_df.count()\n",
    "filtered_count = english_reviews_df.count()\n",
    "percentage_dropped = ((initial_count - filtered_count) / initial_count) * 100\n",
    "\n",
    "print(f\"Percentage of reviews dropped: {percentage_dropped:.2f}%\")\n",
    "\n",
    "# Step 5: Drop the `language` column from the final DataFrame\n",
    "english_reviews_df = english_reviews_df.drop(\"language\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read reviews_with_sentiment.csv\n",
    "reviews_with_sentiment_df = spark.read.csv(\"/Users/okovacsgabor/local_documents/thesis_data/reviews_with_sentiment.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count of menglish_reviews_df after removing duplicates: 167541\n",
      "Row count of merged_df after removing duplicates: 167536\n",
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- incident_id: string (nullable = true)\n",
      " |-- weeks_diff: double (nullable = true)\n",
      " |-- sentiment_score: double (nullable = true)\n",
      "\n",
      "Number of missing sentiment_score values: 11\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on review_id in reviews_with_sentiment_df\n",
    "reviews_with_sentiment_df = reviews_with_sentiment_df.dropDuplicates([\"review_id\"])\n",
    "\n",
    "# Now perform the left join as before\n",
    "merged_df = english_reviews_df.join(\n",
    "    reviews_with_sentiment_df.select(\"review_id\", \"sentiment_score\"),\n",
    "    on=\"review_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Check the row count to confirm\n",
    "print(f\"Row count of menglish_reviews_df after removing duplicates: {english_reviews_df.count()}\")\n",
    "print(f\"Row count of merged_df after removing duplicates: {merged_df.count()}\")\n",
    "\n",
    "# Check the schema to confirm sentiment_score has been added\n",
    "merged_df.printSchema()\n",
    "\n",
    "# Count the number of rows where sentiment_score is missing (i.e., null)\n",
    "missing_sentiment_count = merged_df.filter(merged_df.sentiment_score.isNull()).count()\n",
    "print(f\"Number of missing sentiment_score values: {missing_sentiment_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.coalesce(1).write.mode('overwrite').parquet('final_data_8_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for only complete incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = spark.read.parquet(\"/Users/okovacsgabor/local_documents/thesis_data/clear_process/final_data_8_7.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+---------------+-------------------+--------------------+-----+-----------+----------+---------------+\n",
      "|           review_id|         business_id|     latitude|      longitude|               date|                text|stars|incident_id|weeks_diff|sentiment_score|\n",
      "+--------------------+--------------------+-------------+---------------+-------------------+--------------------+-----+-----------+----------+---------------+\n",
      "|-3rPQVk5g5PPMSa7h...|qY-BUQY-SFBaSrFHo...|39.9550836837| -75.1552917273|2017-02-15 01:01:02|Very good food an...|  5.0|     810756|      -7.0|         0.7574|\n",
      "|-43FFWuLN1g7Gut9r...|XI99CG-lx3OH7oVeR...|36.1517081701| -86.8198794742|2018-02-03 03:28:49|Went for lunch an...|  5.0|    1025673|       3.0|         0.9628|\n",
      "|-4mBqyDxSOkjZ1gib...|yz0KWVamNhqiZGz7X...|29.9545040131| -90.0664367676|2017-05-15 23:14:48|This place was am...|  5.0|     858719|      -3.0|         0.9059|\n",
      "|-50XTL8wXcyfjPK6U...|9PZxjhTIU7OgPIzuG...|39.9497020026| -75.1617702842|2017-02-19 16:13:48|Food and service ...|  5.0|     810756|      -7.0|         0.9022|\n",
      "|-52T0PwCtzmYbFKCN...|wTDsirjG5uWh9go8R...|   36.0990448|   -115.1704519|2017-11-16 00:00:00|Very disappointed...|  2.0|     946496|       6.0|        -0.1986|\n",
      "|-6rnSQdzOAi58e7H-...|DcBLYSvOuWcNReolR...|29.9475628172| -90.0633108616|2018-06-07 15:59:37|Great grilled oys...|  5.0|    1174190|      -8.0|         0.8689|\n",
      "|-9Dwz01C7VCwJyUFq...|7fxebHYUwIF6CakxS...|36.1248436068|-115.1686550577|2017-11-15 18:32:25|Service\\n- excell...|  5.0|     946496|       6.0|            0.0|\n",
      "|-9TCbikQ1H3lErMty...|EjzMHfwqAmj7BgdzQ...|    36.115316|    -115.158502|2017-09-04 00:00:00|Amazing place! Gr...|  5.0|     946496|      -4.0|         0.9656|\n",
      "|-A3A8qjuLzjlMvPxE...|XnQ84ylyAZwh-XfHG...|29.9601574248| -90.0598538142|2018-07-27 13:39:19|Waited in line 30...|  1.0|    1174190|      -1.0|         0.8004|\n",
      "|-AJ4ppzvsGnOG5U9w...|zdE82PiD6wquvjYLy...|    36.179939|    -115.206417|2016-06-18 00:00:00|I love this place...|  5.0|     592207|      -2.0|         0.9792|\n",
      "|-B7U_NTIcaV7er0_a...|IWHdx0NhDKADkGOgX...|  39.95347837| -75.1593960603|2017-03-12 13:26:31|Sooooo this place...|  5.0|     810756|      -4.0|         0.5859|\n",
      "|-C_4WUmiBbCXlYcHE...|uuGL8diLlHfeUeFuo...|    36.102928|    -115.173847|2017-09-12 00:00:00|Came here for Wee...|  3.0|     946496|      -3.0|         0.9149|\n",
      "|-C_hRwDjWONoH8lvU...|N7yuiiu8jhQ-Fl9Np...| 36.106863798|-115.1773988321|2017-10-15 23:34:19|What a nice place...|  5.0|     946496|       2.0|         0.9758|\n",
      "|-ChhrhUDlEbisJmDN...|K7lWdNUhCbcnEvI0N...|    36.109538|     -115.17617|2017-10-18 00:00:00|This is our favor...|  5.0|     946496|       2.0|         0.7096|\n",
      "|-DJ7trN1jhe9uhlOc...|gTC8IQ_i8zXytWSly...|29.9422358659| -90.0670981407|2018-07-05 02:37:32|Had the moroccan ...|  4.0|    1174190|      -4.0|         0.7574|\n",
      "|-G9gCRy7ow97v0Tuy...|CYSPKiVdoPX3erovu...|39.9504170438| -75.1664282754|2019-10-30 01:39:51|Fantastic Happy H...|  5.0|    1537992|       0.0|         0.8221|\n",
      "|-GQCv9FGGKfjMKw-9...|GBTPC53ZrG1ZBY3DT...|29.9507416316| -90.0704155423|2017-06-25 00:16:08|Great atmosphere,...|  5.0|     858719|       3.0|         0.9031|\n",
      "|-Gw-UqtkXE2zTsLn6...|TcNZXteosegb1RO4O...|29.9736823589| -90.0902080718|2017-04-01 03:38:41|These poboys are ...|  5.0|     791274|       3.0|         0.8373|\n",
      "|-ISwYfqrCtcCWjCI1...|77h11eWv6HKJAgojL...|36.1099991328|-115.1742914289|2017-10-18 15:26:15|I couldn't wait t...|  2.0|     946496|       2.0|         0.4712|\n",
      "|-KXgBo9FNLBxixCGC...|YJ8ljUhLsz6CtT_2O...|    36.117417|    -115.175757|2017-08-19 09:57:30|Ugh. This place i...|  2.0|     946496|      -7.0|        -0.9649|\n",
      "+--------------------+--------------------+-------------+---------------+-------------------+--------------------+-----+-----------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Assign a unique ID to each row in all_df\n",
    "all_df_with_id = all_df.withColumn(\"unique_id\", F.monotonically_increasing_id())\n",
    "\n",
    "# Step 2: Group by business_id and incident_id to count distinct weeks_diff for each combination\n",
    "incident_weeks_review_count = (\n",
    "    all_df_with_id\n",
    "    .groupBy(\"business_id\", \"incident_id\")  # Group by both business_id and incident_id\n",
    "    .agg(F.countDistinct(\"weeks_diff\").alias(\"distinct_weeks\"))  # Count distinct weeks for each combination\n",
    ")\n",
    "\n",
    "# Step 3: Filter to include only those pairs that have reviews in all 16 distinct weeks\n",
    "incident_weeks_review_count_filtered = incident_weeks_review_count.filter(F.col(\"distinct_weeks\") == 16)\n",
    "\n",
    "# Step 4: Join the filtered pairs back to all_df_with_id to keep only the relevant rows\n",
    "filtered_all_df = all_df_with_id.join(\n",
    "    incident_weeks_review_count_filtered.select(\"business_id\", \"incident_id\"),\n",
    "    on=[\"business_id\", \"incident_id\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 5: Select the original columns and the unique_id\n",
    "filtered_all_df = filtered_all_df.select(\"unique_id\")\n",
    "\n",
    "# Step 6: Filter the original all_df using the unique_id\n",
    "final_filtered_df = all_df_with_id.join(filtered_all_df, on=\"unique_id\", how=\"inner\").drop(\"unique_id\")\n",
    "\n",
    "# Step 7: Show the result\n",
    "final_filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------------+\n",
      "|distinct_weeks|num_restaurant_incident_pairs|\n",
      "+--------------+-----------------------------+\n",
      "|            16|                          508|\n",
      "+--------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by business_id and incident_id to count distinct weeks_diff for each combination\n",
    "incident_weeks_review_count = (\n",
    "    final_filtered_df\n",
    "    .groupBy(\"business_id\", \"incident_id\")  # Group by both business_id and incident_id\n",
    "    .agg(F.countDistinct(\"weeks_diff\").alias(\"distinct_weeks\"))  # Count distinct weeks for each combination\n",
    ")\n",
    "\n",
    "# Step 2: Group by distinct_weeks and count how many restaurant-incident pairs fall into each category\n",
    "incident_weeks_summary = (\n",
    "    incident_weeks_review_count\n",
    "    .groupBy(\"distinct_weeks\")\n",
    "    .agg(F.count(\"*\").alias(\"num_restaurant_incident_pairs\"))  # Count the number of restaurant-incident pairs for each distinct_weeks\n",
    "    .orderBy(\"distinct_weeks\", ascending=False)  # Sort by number of weeks in descending order\n",
    ")\n",
    "\n",
    "# Step 3: Show the result\n",
    "incident_weeks_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct shooting incidents in the data:\n",
      "30\n",
      "--------------------\n",
      "Distinct businesses in the data:\n",
      "406\n"
     ]
    }
   ],
   "source": [
    "distinct_incident_ids = final_filtered_df.select(countDistinct(\"incident_id\").alias(\"distinct_count\")).collect()[0][\"distinct_count\"]\n",
    "distinct_business_ids = final_filtered_df.select(countDistinct(\"business_id\").alias(\"distinct_count\")).collect()[0][\"distinct_count\"]\n",
    "print(\"Distinct shooting incidents in the data:\")\n",
    "print(distinct_incident_ids)\n",
    "print(\"--------------------\")\n",
    "print(\"Distinct businesses in the data:\")\n",
    "print(distinct_business_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|weeks_diff|\n",
      "+----------+\n",
      "|      -8.0|\n",
      "|      -7.0|\n",
      "|      -6.0|\n",
      "|      -5.0|\n",
      "|      -4.0|\n",
      "|      -3.0|\n",
      "|      -2.0|\n",
      "|      -1.0|\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "|       2.0|\n",
      "|       3.0|\n",
      "|       4.0|\n",
      "|       5.0|\n",
      "|       6.0|\n",
      "|       7.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select distinct weeks_diff and sort them\n",
    "unique_week_diffs = final_filtered_df.select(\"weeks_diff\").distinct().orderBy(\"weeks_diff\")\n",
    "\n",
    "# Show the result\n",
    "unique_week_diffs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45713\n"
     ]
    }
   ],
   "source": [
    "print(final_filtered_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_filtered_df.coalesce(1).write.mode('overwrite').parquet('final_data_8_7_comp_weeks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
